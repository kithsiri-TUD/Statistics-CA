---
title: "multiple_regression_dummy"
author: "kithsiri Jayakody"
date: "12/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

install the relavant packages

```{r}

needed_packages <- c("foreign",  "lm.beta", "stargazer", "car", "ppcor", "userfriendlyscience")                      
# Extract not installed packages
not_installed <- needed_packages[!(needed_packages %in% installed.packages()[ , "Package"])]    
# Install not installed packages
if(length(not_installed)) install.packages(not_installed) 


library(foreign) #To work with SPSS data
library(lm.beta) #Will allow us to isolate the beta co-efficients
library(stargazer)#For formatting outputs/tables

library(userfriendlyscience)#Anova
library(ppcor)#partial correlation
library(car)#Levene's test

```


loading the data and change the type of the data

```{r}
setwd('D:/statistics')
# dataset is loading and converting the character based data types to factors
df = read.csv('sperformance-dataset.csv',stringsAsFactors = F)

# converting the character type into factorial variables and see the conversion
indices <- c(1,2,4:6,9:14,18:23,34,38:43)

for(i in indices)
{
  df[[i]] <- as.factor(df[[i]])
}

```
installing the required libraries 

```{r}
options(repos="https://cran.rstudio.com" )
needed_packages <- c("foreign",  "lm.beta", "stargazer", "car", "ppcor", "userfriendlyscience","olsrr")                      
# Extract not installed packages
not_installed <- needed_packages[!(needed_packages %in% installed.packages()[ , "Package"])]    
# Install not installed packages
if(length(not_installed)) install.packages(not_installed) 


library(foreign) #To work with SPSS data
library(lm.beta) #Will allow us to isolate the beta co-efficients
library(userfriendlyscience)#Anova
library(ppcor)#partial correlation
library(car)#Levene's test
library(olsrr)
library(foreign) #To work with SPSS data
library(lm.beta) #Will allow us to isolate the beta co-efficients
library(stargazer)#For formatting outputs/tables
```





```{r}
#View(df)

#creating the dummy variable names

df$girl = ifelse(df$sex=="F", 1, 0)
#View(df)

```

before I checked the multiple regression, I checked the normal distribution with the mG1 variable and with the pG1 variable.


building the multiple regression model with the dummy variables


```{r}

model1<-lm(df$pG1 ~ df$mG1 + df$girl)
stargazer::stargazer(model1, type="text") #Tidy output of all the required stats

```

v 
A multiple regression analysis was conducted to determine if a student’s maths score and the gender  could predict a student’s protuguese score. 
In order to include the type of genderto the regression model it was recorded into a new  variables girl (1 for girl, 0 for boy). follwoing the reporting of the model.

The F-test is testing if this one variable predicts the exam scores of Portuguese first Grade (pG1) better than if we used the average score of pG1 to predict values for all students. according to the ANOVA It seems it does as it is statistically significant p < 0.001. according to the R2 score, the explainability of the pG1 with the support of mG1 is .457 and the presense of the girl dummy variable  is 1.28  Based on the coefficient we can get the following results. pG1 = 6.48 + .45mG1 where as for the girls it is 
pG1 = (1.28 + 6.48) + .457mG1.


from here I'm going to check the validity of the model.


```{r}

#Influential Outliers - Cook's distance
cooksd<-sort(cooks.distance(model1))
# plot Cook's distance
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels


```
find the rows related to influvential  observations


```{r}

#find rows related to influential observations
influential <- as.numeric(names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))])  # influential row numbers
stem(influential)
head(df[influential, ])  # influential observations.

car::outlierTest(model1) # Bonferonni p-value for most extreme obs - Are there any cases where the outcome variable has an unusual variable for its predictor values?

car::leveragePlots(model1) # leverage plots


```
the access the homocedasticity

```{r}
plot(model1,1)
plot(model1, 3)
```
check the density plot of residuals

```{r}
#A density plot of the residuals
plot(density(resid(model1))) 

```

then finally


```{r}
#Create a QQ plot qPlot(model, main="QQ Plot") #qq plot for studentized resid 
car::qqPlot(model1, main="QQ Plot Model 1") #qq plot for studentized resid

#Collinearity
library(car)


vifmodel<-car::vif(model1)
vifmodel
#Tolerance
1/vifmodel

```
####################################

Reporting of the first model1:

A multiple regression analysis was conducted to determine if a student’s maths score and the gender  could predict a student’s protuguese score. 
In order to include the type of genderto the regression model it was recorded into a new  variables girl (1 for girl, 0 for boy). follwoing the reporting of the model.

The F-test is testing if this one variable predicts the exam scores of Portuguese first Grade (pG1) better than if we used the average score of pG1 to predict values for all students. according to the ANOVA It seems it does as it is statistically significant p < 0.001. according to the R2 score, the explainability of the pG1 with the support of mG1 is .457 and the presense of the girl dummy variable  is 1.28  Based on the coefficient we can get the following results. pG1 = 6.48 + .45mG1 where as for the girls it is 
pG1 = (1.28 + 6.48) + (.457-0.3)mG1.

checking the validity of the model:


A multiple regression analysis was conducted to determine if a student’s score for maths first period results and the dummy varalbe of type sex could predict a student’s protuguese results.

In order to include the type of sex in the regression model it was recorded into two variables girl (0 for Male(M), 1 for Girl(F))

Examination of the histogram, normal P-P plot of standardised residuals and the scatterplot of the dependent variable, and standardised residuals showed that the some outliers existed. However, examination of the standardised residuals showed that none could be considered to have undue influence (95% within limits of -1.96 to plus 1.96 and none with Cook’s distance >1 as outlined in Field (2013). Examination for multicollinearity showed that the tolerance and variance influence factor measures were within acceptable levels (tolerance >0.4, VIF <2.5 ) as outlined in Tarling (2008). The scatterplot of standardised residuals showed that the data met the assumptions of homogeneity of variance and linearity. The data also meets the assumption of non-zero variances of the predictors.  




Generating the second model with the interaction term of girl and the maths first period results.




```{r}

# interaction term is calculated as below.

df$sexInteraction <- df$mG1 * as.numeric(df$girl)

model2<-lm(df$pG1 ~ df$mG1 + df$girl + df$sexInteraction)
stargazer::stargazer(model2, type="text") #Tidy output of all the required stats


```


A multiple regression analysis was conducted to determine if a student’s maths score and the gender and the interaction term gender with the maths first grade score  could predict a student’s protuguese score. 
In order to include the type of gender to the regression model it was recorded into a new  variables girl (1 for girl, 0 for boy). follwoing the reporting of the model.

The F-test is testing if this one variable predicts the exam scores of Portuguese first Grade (pG1) better than if we used the average score of pG1 to predict values for all students. according to the ANOVA It seems it does as it is statistically significant p < 0.001. according to the R2 score, the explainability of the pG1 with the support of mG1 is .443 and the presense of the girl dummy variable  is 0.957  Based on the coefficient we can get the following results.with the maths first grade and with the girls
dummy variable and with the interaction term of girl and the maths first period results it is 
pG1 = (1.28 + 6.48) + (.443-0.3)mG1.  


after generating the model , here with I'm going to check the assumption to validate the model.

```{r}

plot(model2)# Plots - first and third are the ones of interest for homocedasticity


#Influential Outliers - Cook's distance
cooksd<-sort(cooks.distance(model2))
# plot Cook's distance
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels


#find rows related to influential observations
influential <- as.numeric(names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))])  # influential row numbers
stem(influential)
head(df[influential, ])  # influential observations.


car::outlierTest(model2) # Bonferonni p-value for most extreme obs - Are there any cases where the outcome variable has an unusual variable for its predictor values?

car::leveragePlots(model2) # leverage plots


#Create histogram and a density plot of the residuals
plot(density(resid(model2))) 

#Create a QQ plot qPlot(model, main="QQ Plot") #qq plot for studentized resid 
car::qqPlot(model2, main="QQ Plot Model 2") #qq plot for studentized resid


#Collinearity
vifmodel<-car::vif(model2)
vifmodel
#Tolerance
1/vifmodel

```

checking 
```{r}

#Collinearity
vifmodel<-car::vif(model2)
vifmodel
#Tolerance
1/vifmodel

```

A multiple regression analysis was conducted to determine if a student’s score for maths first period results and the dummy varalbe of type sex and the interaction term
 of girl and the maths first grade results could predict a student’s protuguese results.

In order to include the type of sex in the regression model it was recorded into two variables girl (0 for Male(M), 1 for Girl(F)) and the interaction term was calculated with girl * maths first grade resullts.

Examination of the histogram, normal P-P plot of standardised residuals and the scatterplot of the dependent variable, and standardised residuals showed that the some outliers existed. However, examination of the standardised residuals showed that none could be considered to have undue influence (95% within limits of -1.96 to plus 1.96 and none with Cook’s distance >1 as outlined in Field (2013). Examination for multicollinearity showed that the tolerance and variance influence factor measures were not within acceptable levels (tolerance >0.4, VIF <2.5 ) as outlined in Tarling (2008). therefore we cannot accept this model for determining the students first period protuguese results with maths first period results and with girls dummy variable and with the interaction term.

---
title: "Dimensionality_Reduction_PCA"
author: "kithsiri Jayakody"
date: "12/9/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

loading the relavant files

```{r}

needed_packages <- c("psych",  "REdaS", "Hmisc", "corrplot", "ggcorrplot", "factoextra",  "nFactors")                      
# Extract not installed packages
not_installed <- needed_packages[!(needed_packages %in% installed.packages()[ , "Package"])]    
# Install not installed packages
if(length(not_installed)) install.packages(not_installed, repos = "http://cran.us.r-project.org") 
library(psych)
library(REdaS)
library(Hmisc)
library(corrplot)
library(ggcorrplot)
library(factoextra)#Used for principal component analysis to get a different view of eigenvalues
library(nFactors)



```



loading the file  for the anaysis (this dataset included The results each student achieved on a 50-item IPIP Big-Five personality inventory has been included.).
The IPIP items were administered using a 5-point, Likert-type scale ranging from 1 (disagree) to 5 (agree) as in the original instrument. This has resulted in 50 extra variables being added for each row. Each variable represents the student’s answer from 1 to 5 for each question or 0 if the question was not answered

```{r}
#load the dataset and remove every row when students didn't answer a question

setwd('D:/statistics')
# dataset is loading 
df = read.csv('studentpIusepersonality.csv',stringsAsFactors = F)



```



getting only the required columns for the analysis

```{r}

# select only the required columns for the analysis . columns start with 55 to 197.
df1 = df[,c(55:64,78:87,98:107,188:197,198:207)]
# converting the character type into factorial variables and see the conversion
View(df1)



```




```{r}
#remove any row which has the value = 0 . it means it is not answered.
df2 <- na.omit(df1)


```


check corelation matrix ( it will shows a 50 * 50 matrix)

```{r}
#create a correlation matrix (these are just some methods)
raqMatrix<-cor(df2)
round(raqMatrix, 2)
Hmisc::rcorr(as.matrix(df2))

```

as the out put there is not clear, here with I'm creating a ggplot matrix for clear visualization

```{r}

#Using ggcorrplot. Note these are examples you need to choose a style for yourself, you do not need to create multiple correlation matrices
p.mat <- ggcorrplot::cor_pmat(df2)
ggcorrplot::ggcorrplot(raqMatrix, title = "Correlation matrix for RAQ data")
#Showing Xs for non-significant correlations
ggcorrplot::ggcorrplot(raqMatrix, title = "Correlation matrix for RAQ data", p.mat = p.mat, sig.level = .05)
#Showing lower diagonal
ggcorrplot::ggcorrplot(raqMatrix, title = "Correlation matrix for RAQ data", p.mat = p.mat, sig.level = .05, type="lower")


#Overlay plot with a white grid to space things out.
#t1.cex is the text size, pch is controlling what is shown for non-significant correlations
ggcorrplot(raqMatrix, sig.level=0.05, lab_size = 4.5, p.mat = NULL,
           insig = c("pch", "blank"), pch = 1, pch.col = "black", pch.cex =1,
           tl.cex = 10) +
  theme(axis.text.x = element_text(margin=margin(-2,0,0,0)),
        axis.text.y = element_text(margin=margin(0,-2,0,0)),
        panel.grid.minor = element_line(size=10)) + 
  geom_tile(fill="white") +
  geom_tile(height=0.8, width=0.8)


#Showing the co-coefficients (this will be messy given the number of variables)
ggcorrplot::ggcorrplot(raqMatrix, lab=TRUE, title = "Correlation matrix for RAQ data",  type="lower")


```
using the corelation plots

```{r}

#Visualization of correlations using circles
#corrplot parameters method = c("circle", "square", "ellipse", "number", "shade",
#"color", "pie")
#type = c("full", "lower", "upper"),
corrplot::corrplot(raqMatrix, method="circle")
corrplot::corrplot(raqMatrix, method="circle", type="upper")
#Visualization using numbers
corrplot::corrplot(raqMatrix, method="number")

#Visualization of significance levels at 0.05
res1 <- corrplot::cor.mtest(raqMatrix, conf.level = .95)
corrplot::corrplot(raqMatrix, p.mat = res1$p, type="lower", sig.level = .05)

#Showing p-value for non-significant results
corrplot(raqMatrix, p.mat = res1$p, type="lower",insig = "p-value")

```



Check if data is suitable - look at the relevant Statistics
Bartlett's test

```{r}
psych::cortest.bartlett(df2)
psych::cortest.bartlett(raqMatrix, n=nrow(df2))
```

execute for 

```{r}
#KMO (execute one of these):
REdaS::KMOS(df2)
psych::KMO(df2)

```


According to the analysis we can observe that most of the values are above , .8. Indicates the proportion of variance in these variables  caused by an underlying factor. Moreover according to the bartlett, it is evidant that the p value is <0.05.

checking the determinent

```{r}

###Determinant
#Determinant (execute one of these):
det(raqMatrix)
det(cor(df2))

```
as indicated with this figure determinent value is higher than .00001.  so there represent multicollinearity.


perform the dimension reduction

```{r}

#pcModel<-principal(dataframe/R-matrix, nfactors = number of factors, rotate = "method of rotation", scores = TRUE)

#On raw data using principal components analysis
#For PCA we know how many factors if is possible to find
#principal will work out our loadings of each variable onto each component, the proportion each component explained and the cumulative proportion of variance explained 
pc1 <-  principal(df2, nfactors = 50, rotate = "none")
pc1 <-  principal(df2, nfactors = length(df2), rotate = "none")
pc1#output all details of the PCA

```

step 4 : decide which components to retain here 


```{r}

#Create the scree plot
plot(pc1$values, type = "b") 
#Print the variance explained by each component
pc1$Vaccounted 
#Print the Eigenvalues
View(pc1$values)

```


#Another way to look at eigen values plus variance explained (need to use princomp function of PCA to get right class for use with factoextra functions)

```{r}

pcf=princomp(df2)
factoextra::get_eigenvalue(pcf)
factoextra::fviz_eig(pcf, addlabels = TRUE, ylim = c(0, 50))#Visualize the Eigenvalues
factoextra::fviz_pca_var(pcf, col.var = "black")
factoextra::fviz_pca_var(pcf, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )

#Print the loadings above the level of 0.3
psych::print.psych(pc1, cut = 0.3, sort = TRUE)
#create a diagram showing the components and how the manifest variables load
fa.diagram(pc1) 
#Show the loadings of variables on to components
fa.sort(pc1$loading)
#Output the communalities of variables across components (will be one for PCA since all the variance is used)
pc1$communality 
#Visualize contribution of variables to each component
var <- factoextra::get_pca_var(pcf)
corrplot::corrplot(var$contrib, is.corr=FALSE) 

# Contributions of variables to PC1
factoextra::fviz_contrib(pcf, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
factoextra::fviz_contrib(pcf, choice = "var", axes = 2, top = 10)

```



# factor rotations

```{r}

#Apply rotation to try to refine the component structure
pc2 <-  principal(df2, nfactors = 6, rotate = "varimax")#Extracting 4 factors
#output the components
psych::print.psych(pc2, cut = 0.3, sort = TRUE)
#output the communalities
pc2$communality
#NOTE: you can do all the other things done for the model created in pc1

```



Do the dimension reduction and Step 4: Decide which factors/components to retain (FACTOR ANALYSIS)



```{r}

#Factor Analysis - the default here is principal axis factoring fm=pa
#If we know our data going in is normally distributed we use maximum likelihood
facsol <- psych::fa(raqMatrix, nfactors=6, obs=NA, n.iter=1, rotate="varimax", fm="pa")

#Create your scree plot
plot(facsol$values, type = "b") #scree plot

#Print the Variance accounted for by each factor/component
facsol$Vaccounted
#Output the Eigenvalues
facsol$values 

#Print the components with loadings
psych::print.psych(facsol,cut=0.3, sort=TRUE)

#Print sorted list of loadings
fa.sort(facsol$loading)

#create a diagram showing the factors and how the manifest variables load
fa.diagram(facsol)

#Note: you can apply rotation as you did for PCA

```






A principal component analysis (PCA) was conducted on the 50  items with orthogonal rotation (varimax).  Bartlett’s test of sphericity, Χ2(253) = 8165.92, p< .001, indicated that correlations between items were sufficiently large for PCA.  An initial analysis was run to obtain eigenvalues for each component in the data.  eleven components had eigenvalues over Kaiser’s criterion of 1 and in combination explained 30.43% of the variance.  The scree plot was slightly ambiguous and showed inflexions that would justify retaining either 6 or 9 factors.  
Given the large sample size, and the convergence of the scree plot and Kaiser’s criterion on six components, six components were retained in the final analysis. 



`